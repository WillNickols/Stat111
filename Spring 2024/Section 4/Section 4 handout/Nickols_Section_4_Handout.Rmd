---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 4}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/BS4XXnzdR53ZhxDS9).

Pset 4 due...

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
```

# Kahneman (Warm-up)

Without looking anything up, give 90\% confidence intervals for the following quantities: (These are reproduced from Russo and Schoemaker 1989 but the answers are updated.)

| Statement                                                               | Lower | Upper |
|-------------------------------------------------------------------------|-------|-------|
| Martin Luther King Jr.'s age at death                                   | _____ | _____ |
| Length of the Nile River                                                | _____ | _____ |
| Number of countries that are members of OPEC                            | _____ | _____ |
| Number of books in the Catholic Old Testament                           | _____ | _____ |
| Diameter of the moon                                                    | _____ | _____ |
| Weight of an empty Boeing 747                                           | _____ | _____ |
| Year in which Wolfgang Amadeus Mozart was born                          | _____ | _____ |
| Gestation period (in days) of an Asian elephant                         | _____ | _____ |
| Air distance from London to Tokyo                                       | _____ | _____ |
| Deepest recorded point in the oceans                                    | _____ | _____ |
| Proportion of people taking this quiz in the room who got at least 8/10 | _____ | _____ |

# Brr

The following questions deal with the 2000-2022 temperatures of Boston and Chicago [available here](https://www.ncei.noaa.gov/cdo-web).

```{r, echo=F, cache=F, fig.height=3.5, warning=F}
temps <- read.csv("data/temperatures.csv")
temps$Date <- as.Date(temps$Date)
p1 <- ggplot(temps[temps$Date > as.Date("2017-01-01"),], 
             aes(x = Date, y = Min, col = City, shape = City)) + 
  geom_point(size=2, alpha=0.3) + 
  theme_bw() + 
  theme(text = element_text(size = 12),
        legend.position = "bottom",
        legend.direction = "horizontal") + 
  ylab("Daily Minimum Temperature (C)")

temps_diff <- temps[temps$City == "Boston",]
temps_diff$diff <- temps_diff$Min - temps$Min[temps$City == "Chicago"]
temps_diff <- temps_diff[!is.na(temps_diff$diff),]
p2 <- ggplot(temps_diff, aes(x = diff)) + 
  geom_histogram(bins = 28) + 
  theme_bw() + 
  xlab("Difference in Mins") + 
  ylab("Count")

grid.arrange(p1, p2, ncol=2, widths = c(1.6,1))
```

In this problem, we'd like to determine whether daily minimum temperatures are significantly different between Boston and Chicago. To do this, we'll explore the student-$t$ distribution and confidence intervals. Our strategy will be to find the null distribution of some statistic assuming the true difference is 0 and then see how likely we are to have observed the crystallized version of that statistic under the null.

1. Let $Y_1,...,Y_n\sim\mathcal{N}(0,\sigma^2)$ i.i.d. with $n=8091$. Find the distribution of $\bar{Y}$ assuming $\sigma^2$ is known, and use that to give a standardized distribution for $\bar{Y}$.

\vspace{2 cm}

2. Let $S^2=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})^2$ be the sample variance. Then, by 10.4.3 in the Stat 110 book, $\frac{(n-1)S^2}{\sigma^2}\sim\chi_{n-1}^2$. Show that the sample variance is independent of the sample mean by using facts about Multivariate Normals and the vector $(\bar{Y}, Y_1-\bar{Y},...,Y_n-\bar{Y})$. (Hint: In a MVN vector, zero covariance implies independence. Also, a function of a random vector independent of a random variable is also independent of the random variable.)

\vspace{5 cm}

3. Using the results above, write a function of $\bar{Y}$ and $S^2$ that has the $t_{n-1}$ distribution (this is our pivot). Recall that the $t_n$ distribution is defined as $\frac{Z}{\sqrt{V/n}}$ where $Z\sim\mathcal{N}(0,1)$ and $V\sim\chi_{n}^2$ are independent.

\vspace{4 cm}

4. In terms of a CDF $F$, determine $P\left(\frac{\bar{Y}}{\sqrt{S^2/n}}>\tau\right)$ for a fixed $\tau$. Describe what this probability means in the context of the problem.

\vspace{2 cm}

5. Using the pivot, find a $95\%$ confidence interval for $0$ and interpret what this means.

\vspace{4 cm}

6. The following is the previous interval computed from the data. Based on the interval, comment on whether it seems likely that the data follow the stated distribution (i.e. that the true mean difference is 0).

```{r, echo=F}
n <- dim(temps_diff)[1]
lb = qt(0.025, n-1) * sqrt(var(temps_diff$diff)/n) - mean(temps_diff$diff)
ub = qt(0.975, n-1) * sqrt(var(temps_diff$diff)/n) - mean(temps_diff$diff)
outputs <- c(lb, ub)
names(outputs) <- c("Lower bound", "Upper bound")
outputs
```

\vspace{2 cm}

7. Show that $t_n\xrightarrow{d} Z$ with $Z\sim\mathcal{N}(0,1)$ as $n\rightarrow\infty$. (Hint: write the denominator as a sum of squared random variables and apply asymptotic tools.)

\vspace{4 cm}

8. How close is the $t_{n-1}$ interval above to an interval using the Standard Normal? Why?

```{r, echo=F}
n <- dim(temps_diff)[1]
lb = qnorm(0.025) * sqrt(var(temps_diff$diff)/n) - mean(temps_diff$diff)
ub = qnorm(0.975) * sqrt(var(temps_diff$diff)/n) - mean(temps_diff$diff)
outputs <- c(lb, ub)
names(outputs) <- c("Lower bound", "Upper bound")
outputs
```

\vspace{1 cm}

9. Now, assume we have $Y_i\sim\mathcal{N}(\mu,\sigma^2)$ with $\sigma^2$ known. We have seen before that $\bar{Y}$ is the unbiased MLE for $\mu$. Does $\bar{Y}$ achieve the Cramér-Rao lower bound?  (The Cramér-Rao lower bound is the reciprocal of the Fisher information for the dataset for $\mu$.)

\vspace{5 cm}

\newpage
# Uniformity

1. [One fast way](https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution) of computing Normal-like random variables is to take the sum of 12 $\textrm{Unif}(0,1)$ random variables and subtract $6$. Find the expectation and variance of the resulting distribution.

```{r, fig.height=2.5, fig.width=5, fig.align='center', echo=F}
df = data.frame(x = replicate(100000, sum(runif(12)) - 6))
ggplot(df, aes(x = x)) + 
  geom_histogram(bins=30, aes(y = after_stat(density))) + 
  stat_function(fun = dnorm, args = c(0, 1), n = 100, col = "red") + 
  theme_bw() + 
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

\vspace{4 cm}

2. Now, suppose you have $n$ $\textrm{Unif}(\alpha-\beta/2, \alpha+\beta/2)$ i.i.d. random variables. Write the likelihood function for $\alpha$ and $\beta$.

\vspace{3 cm}

3. Consider trying to fine the MLE estimates with numerical optimization; that is, choose a starting point and iteratively evaluate the gradient of the log-likelihood and take a small step in that direction until the parameters stop updating. What might go wrong computationally?

\vspace{5 cm}

5. Another set of estimators is $\hat{\alpha}=Y_{(n/2+1/2)}$ and $\hat{\beta}=2(Y_{(n/2+1/2)}-Y_{(1)})$. Describe the logic of these estimators and then find their biases and variances. Assume $n$ is odd. (Note that $U_{(k)}-U_{(j)}\sim\textrm{Beta}(k-j, n-(k-j)+1)$ for Standard Uniforms.)

\vspace{10 cm}

6. Make the $\hat{\beta}$ estimator unbiased.

\vspace{2 cm}












