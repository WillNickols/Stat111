---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 3}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/BS4XXnzdR53ZhxDS9).

Pset 3 due 2/16.

```{r, echo=F, warning=F, message=F, cache=T}
list.of.packages <- c("ggplot2", "gridExtra")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
```

# Stonks

The following questions deal with the past 5 years of S&P 500 adjusted closing prices [available here](https://finance.yahoo.com/quote/%5EGSPC/history?period1=1518307200&period2=1676073600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true).

```{r, echo=F, cache=T, fig.height=3.5}
stocks <- read.csv("data/stocks.csv")

stocks$year <- gsub('-.*', '', stocks$Date)
stocks$month <- gsub("^.*-(.*)-.*$", "\\1", stocks$Date)
stocks$day <- gsub(".*-", "", stocks$Date)

stocks$close_diff <- (stocks$Close - stocks$Open) / stocks$Open
# Convert to percent
stocks$close_diff <- stocks$close_diff * 100

# Convert dates into days since Feb 12 2018
stocks$time_step <- as.Date(apply(stocks[,c("year", "month", "day")], 1, paste, collapse = "-"))

p1 <- ggplot(stocks, aes(x=abs(close_diff))) + 
  geom_histogram(breaks = seq(0, 6, 0.2)) + 
  theme_bw() + 
  ylab("Days") + 
  xlab("Absolute percent difference\nin adjusted closing price")

p2 <- ggplot(stocks, aes(x = time_step, y = close_diff)) + 
  geom_point() + 
  theme_bw() + 
  ylab("Percent difference\nin adjusted closing price") + 
  xlab("Year") +
  scale_x_date(date_breaks = "years" , date_labels = "%Y")

grid.arrange(p1, p2, ncol=2)
```

In this section, we will be modeling the day-to-day absolute percent differences in the adjusted closing price of the S&P 500 as $Y_1,...,Y_n\sim\textrm{Expo}(\lambda)$.

1. Find the score of $\lambda$ (the score is $\frac{\partial}{\partial \lambda}\ell(\lambda; \vv{y})$) in terms of the sample mean and verify that $E(s(\lambda^*;\vv{Y}))=0$. (This equality was the last part of Neil's Thursday lecture.)

\vspace{5 cm}

2. Show that the MLE of $\hat{\lambda}$ is consistent for $\lambda$. That is, show that $\hat{\lambda}\rightarrow \lambda$ as $n\rightarrow\infty$ by showing the MSE goes to 0, a LLN holds, making a claim using the CMT, or showing convergence directly.

\vspace{3 cm}

3. In his book *The Black Swan*, Nassim Taleb argues that part of the reason for the 2008 financial crisis was a failure to model market fluctuations and assign sufficient probability to extreme events. Let us consider daily absolute differences above $\tau$ to be extreme events. Let $X_i=I(Y_i>\tau)$. Show that $\bar{X}$ is consistent for $p=P(Y_i>\tau)$ first by using MSE and then by using the law of large numbers. 

\vspace{6 cm}

4. Find the asymptotic distribution of $\bar{X}$ and its approximate distribution for large $n$ in terms of $\lambda$.

\vspace{5 cm}

5. Now, suppose we estimate $P(Y_i>\tau)$ with $\hat{p}=e^{-\hat{\lambda}\tau}$. Given that $$\sqrt{n}(\hat{\lambda}-\lambda^*)\rightarrow\mathcal{N}\left(0, \lambda^{*2}\right)$$ (we'll see how to find this in future weeks), find the asymptotic distribution of $\hat{p}$ and its approximate distribution for large $n$.

\vspace{7 cm}

6. The distributions in 4 and 5 should have the same mean. However, the variances are different. The following are the estimated standard errors of each estimator with $\tau=5$. Explain why the results are what they are.

```{r, echo=F}
lambda_hat <- 1/mean(abs(stocks$close_diff))
n <- length(stocks$close_diff)
tau = 5
round(c("Xbar SE" = sqrt(exp(-lambda_hat * tau) * (1-exp(-lambda_hat * tau)) / n), 
  "phat SE" = sqrt(tau^2 * exp(-2 * lambda_hat * tau) * lambda_hat^2) / n), digits = 6)
```

\vspace{3 cm}

7. Though $\hat{p}$ is more efficient, it might be less robust. The following shows the MSEs of the two estimators for estimating $P(Y>\tau)$ when $Y\sim\textrm{Expo}(0.5)$ (the correct model) and $Y\sim\textrm{Log-Normal}(0.1, 1)$ (an incorrect model). Again, we have used $\tau=5$ and $n=100$ with $10^5$ simulations.

```{r, cache=T, echo=F}
nsims <- 10^5
n <- 100
tau <- 5
mse_xbar_lnorm <- vector(length = nsims)
mse_phat_lnorm <- vector(length = nsims)
mse_xbar_exp <- vector(length = nsims)
mse_phat_exp <- vector(length = nsims)
for (i in 1:nsims) {
  log_norms <- rlnorm(n, 0.1, 1)
  mse_xbar_lnorm[i] <- (mean(log_norms > tau) - plnorm(tau, 0.1, 1, lower.tail = F))^2
  mse_phat_lnorm[i] <- (exp(-1/mean(log_norms) * tau) - plnorm(tau, 0.1, 1, lower.tail = F))^2
  
  expos <- rexp(n, 0.5)
  mse_xbar_exp[i] <- (mean(expos > tau) - pexp(tau, 0.5, lower.tail = F))^2
  mse_phat_exp[i] <- (exp(-1/mean(expos) * tau) - pexp(tau, 0.5, lower.tail = F))^2
}

output <- rbind(c(mean(mse_xbar_lnorm), mean(mse_phat_lnorm)),
      c(mean(mse_xbar_exp), mean(mse_phat_exp)))
colnames(output) <- c("Xbar", "phat")
rownames(output) <- c("Log Normal", "Expo")
round(output, digits = 5)
```

\vspace{3 cm}

\newpage

# Ty Mup

Ty Mup is taking an exam with $n$ equally hard questions. He has a probability $p_2$ of getting each question right independently. However, there is also a $0<p_1<1$ probability he sleeps through his alarm and misses the exam entirely. Let $Y$ be the number of questions he gets right on his exam. (This distribution is called the zero-inflated binomial.)

1. Find $E(Y|Y>0)$.

\vspace{6 cm}

2. Unfortunately, [the day is February 2nd in Punxsutawney](https://en.wikipedia.org/wiki/Groundhog_Day_(film)) and Ty is destined to repeat this day $d$ times, scoring i.i.d $Y_i$ on the exams. Find the likelihood function, the log-likelihood function, and the score for $p_1$ and $p_2$. (Hint: Let $m$ be the number of $0$s.)

\vspace{10 cm}

3. Find a two-dimensional sufficient statistic (a two dimensional statistic that contains all the information about the likelihood).

\vspace{1 cm}

4. Let $B$ be the event that Ty sleeps through the alarm at least once. Show that as $d\rightarrow\infty$,
$$I_B\frac{d^{1/2}(\bar{Y}-(1-p_1)np_2)}{\sqrt{np_2(1-p_2)(1-p_1)+(np_2)^2p_1(1-p_1)}}\xrightarrow{d}\mathcal{N}(0,1)$$
\vspace{10 cm}







