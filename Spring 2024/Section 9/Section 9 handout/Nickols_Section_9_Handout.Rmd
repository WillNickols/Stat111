---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 9}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/2PA9A94GpdmSDH2J9) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 9 due Friday 4/16

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra", "reshape2", "matrixStats")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
library(reshape2)
library(matrixStats)
```

# Optimal Polling

When conducting political polls, a reasonable goal is to construct the most accurate and precise estimate of public opinion while contacting as few people as possible.  Each person you contact will require some amount of time and labor, so minimizing this count is preferable.  In this problem, we will be looking at binary voter support of some candidate where voter $i$ has the indicator $y_i$ which is 1 if the voter supports the candidate and $0$ otherwise.  We assume that everyone in the size $N$ population has an opinion about the candidate, so the $y_i$ are not random, but which of these we actually sample is random.  We want to estimate $\mu=\frac{1}{N}\sum_{i=1}^Ny_i$, the average support for the candidate.

In the United States, 31 states, the District of Columbia, and the U.S. Virgin Islands allow voters to indicate their partisan affiliations on voter registration forms and also report these total registration numbers publicly.  Suppose for this problem that someone can only be a registered Democrat, registered Republican, or independent, and suppose we have information on which of these groups each person is.  Note that someone's voter registration does not constrain whom the voter can vote for.  For example, a registered Democrat can vote for a Republican.

1. Suppose we are in a state that makes this voter registration public so we can look up someone's party affiliation if the person has one.  Explain intuitively why it might be suboptimal to take a simple random sample of voters, contact them, and average their opinions on the candidate.

\vspace{2 cm}

2. Let $\mu$ be the support for the candidate in the whole state.  Let $\mu_1=\frac{1}{N_1}\sum_{i:\textrm{Voter i is a Democrat}}y_i$ be the support for the candidate among all registered Democrats, $\mu_2$ be the support among all registered Republicans, and $\mu_3$ be the support among all independents.  Let $p_1$ be the proportion of Democrats in the state, $p_2$ be the proportion of Republicans, and $p_3$ be the proportion of independents.  We treat these $p$ as known since we can look up the registered proportions.  Suppose a random sample of $n$ people is taken without replacement from the state and the people are contacted about their opinions on the candidate.  Let $Y_1,...,Y_n$ be the opinions reported in the sample.  Find the bias and variance of $$\hat{\mu}=\frac{1}{n}\sum_{i=1}^nY_i$$ in terms of the $\mu$s and $p$s.  Recall that the variance of the sample average with a finite population correction is $\frac{\textrm{Var}(Y_1)}{n}\frac{N-n}{N-1}$.

\vspace{5 cm}

3. Now, consider the stratified estimator $$\tilde{\mu}=\sum_{l=1}^3p_l\bar{Y}_l$$ with sample size $n_l$ for strata $l$.  Find its bias and variance in terms of the $\mu$s, $p$s, and $n$s.

\vspace{5 cm}

4. Using the fact that the optimal subsample size is $n_l/n\propto N_l\sigma_l$, find $n_l$ as a function of $n$, the $N_l$s, and the $\mu_l$s.

\vspace{4 cm}

5. Explain why the answer above is still useful even though we have $\mu_l$s in it.

\vspace{3 cm}

\newpage

6. For $N=10^6, n=1000, p_1=0.25, p_2=0.3, p_3=0.45, \mu_1=0.9, \mu_2=0.1, \mu_3=0.55$, the following plot shows the standard deviation of each estimator.  One variable at a time is varied in each plot.  Explain why these results make sense.

```{r, echo=F, warning=F, fig.align='center', fig.width=7, fig.height=5, cache=T}
# Don't mind this monstrosity
N <- 10^6
n <- 1:10000
p1 <- 0.25
p2 <- 0.3
p3 <- 0.45
mu_1 <- 0.9
mu_2 <- 0.1
mu_3 <- 0.55

nl1_tmp <- n * sqrt(mu_1 * (1-mu_1)) * p1 * N
nl2_tmp <- n * sqrt(mu_2 * (1-mu_2)) * p2 * N
nl3_tmp <- n * sqrt(mu_3 * (1-mu_3)) * p3 * N
nl1 <- nl1_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n
nl2 <- nl2_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n
nl3 <- nl3_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n

var_1 <- (p1 * mu_1 * (1-mu_1) + p2 * mu_2 * (1-mu_2) + p3 * mu_3 * (1-mu_3) + mu_1^2 * p1 + mu_2^2*p2+mu_3^2*p3 - (mu_1 * p1+mu_2 * p2 + mu_3 * p3)^2) / n * (N-n)/(N-1)

var_2 <- p1^2 * (mu_1 * (1 - mu_1)) / nl1 * (p1 * N - nl1) / (p1 * N - 1) + 
  p2^2 * (mu_2 * (1 - mu_2)) / nl2 * (p2 * N - nl2) / (p2 * N - 1) + 
  p3^2 * (mu_3 * (1 - mu_3)) / nl3 * (p3 * N - nl3) / (p3 * N - 1)

df <- data.frame(x=1:10000, sqrt(var_1), sqrt(var_2))
df <- melt(df, id.vars=c("x"))
df$variable <- ifelse(df$variable == "sqrt.var_1.", "SRS (No replacement)", "Stratified")

plot1 <- ggplot(df, aes(x = x, y = value, col = variable)) + 
  geom_line() + 
  scale_y_continuous(trans = "log", breaks = c(0.005, 0.01, 0.025, 0.1, 0.25)) + 
  scale_x_continuous(trans = "log", breaks = c(3, 10, 30, 100, 300, 1000, 3000, 10000)) + 
  theme_bw() + 
  theme(legend.position = "none") + 
  labs(col="Strategy") + 
  ylab("Standard deviation") + 
  xlab("Sample size")

N <- 10^6
n <- 1000
p1 <- 0.25
p2 <- 0.3
p3 <- 0.45
mu_1 <- 0.9
mu_2 <- 0.1
mu_3 <- seq(0.25, 0.75, 0.001)

nl1_tmp <- n * sqrt(mu_1 * (1-mu_1)) * p1 * N
nl2_tmp <- n * sqrt(mu_2 * (1-mu_2)) * p2 * N
nl3_tmp <- n * sqrt(mu_3 * (1-mu_3)) * p3 * N
nl1 <- nl1_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n
nl2 <- nl2_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n
nl3 <- nl3_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n

var_1 <- (p1 * mu_1 + p2 * mu_2 + p3 * mu_3 - (mu_1 * p1+mu_2 * p2 + mu_3 * p3)^2) / n * (N-n)/(N-1)

var_2 <- p1^2 * (mu_1 * (1 - mu_1)) / nl1 * (p1 * N - nl1) / (p1 * N - 1) + 
  p2^2 * (mu_2 * (1 - mu_2)) / nl2 * (p2 * N - nl2) / (p2 * N - 1) + 
  p3^2 * (mu_3 * (1 - mu_3)) / nl3 * (p3 * N - nl3) / (p3 * N - 1)

df <- data.frame(x=mu_3, sqrt(var_1), sqrt(var_2))
df <- melt(df, id.vars=c("x"))
df$variable <- ifelse(df$variable == "sqrt.var_1.", "SRS (No replacement)", "Stratified")

plot2 <- ggplot(df, aes(x = x, y = value, col = variable)) + 
  geom_line() + 
  theme_bw() + 
  theme(legend.position = "none") + 
  labs(col="Strategy") + 
  ylab("Standard deviation") + 
  xlab(expr(mu))

N <- 10^6
n <- 1000
p1 <- 0.25/0.55 * (1 - seq(0, 1, 0.001))
p2 <- 0.3/0.55 * (1 - seq(0, 1, 0.001))
p3 <- seq(0, 1, 0.001)
mu_1 <- 0.9
mu_2 <- 0.1
mu_3 <- 0.55

nl1_tmp <- n * sqrt(mu_1 * (1-mu_1)) * p1 * N
nl2_tmp <- n * sqrt(mu_2 * (1-mu_2)) * p2 * N
nl3_tmp <- n * sqrt(mu_3 * (1-mu_3)) * p3 * N
nl1 <- nl1_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n
nl2 <- nl2_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n
nl3 <- nl3_tmp / (nl1_tmp + nl2_tmp + nl3_tmp) * n

var_1 <- (p1 * mu_1 * (1-mu_1) + p2 * mu_2 * (1-mu_2) + p3 * mu_3 * (1-mu_3) + mu_1^2 * p1 + mu_2^2*p2+mu_3^2*p3 - (mu_1 * p1+mu_2 * p2 + mu_3 * p3)^2) / n * (N-n)/(N-1)

var_2 <- p1^2 * (mu_1 * (1 - mu_1)) / nl1 * (p1 * N - nl1) / (p1 * N - 1) + 
  p2^2 * (mu_2 * (1 - mu_2)) / nl2 * (p2 * N - nl2) / (p2 * N - 1) + 
  p3^2 * (mu_3 * (1 - mu_3)) / nl3 * (p3 * N - nl3) / (p3 * N - 1)

df <- data.frame(x=p3, sqrt(var_1), sqrt(var_2))
df <- melt(df, id.vars=c("x"))
df$variable <- ifelse(df$variable == "sqrt.var_1.", "SRS (No replacement)", "Stratified")

plot3 <- ggplot(df, aes(x = x, y = value, col = variable)) + 
  geom_line() + 
  theme_bw() + 
  labs(col="Strategy") + 
  ylab("Standard deviation") + 
  xlab("p_3") + 
  theme(legend.position = "bottom")

grid.arrange(plot1, plot2, plot3, ncol=2, heights = c(1, 1.2))
```

\vspace{3 cm}

7. Create a Horvitz-Thompson estimator for the SRS case and stratified sampling case.  Are either equivalent to the corresponding estimators above?

\vspace{4 cm}

\newpage
# Realistic conjugacy

In Normal-Normal conjugacies, we often assume both the group mean's variance and the observation's variance are known.  However, there are very few examples where this is actually the case.  To ameliorate this, we can either switch to a more complicated conjugate prior (e.g. Normal-Gamma) or use asymptotics.  Here, we will do the latter.  Suppose we have $\mu_i\sim\mathcal{N}(\mu_0,\sigma^2_0)$ and $Y_{i,j}\sim[\mu_i,\sigma^2_i]$ i.i.d. conditional on $\mu_i$.  This notation means $Y_{i,j}$ has mean $\mu_i$ and variance $\sigma^2_i$ conditional on $\mu_i$ and $\sigma^2_i$, but it does not place distributional assumptions on $Y_{i,j}$.

1. Conditional on $\mu_j$, show that $$\sqrt{n_i}\left(\frac{\bar{Y}_{i}-\mu_i}{\hat{\sigma_i}}\right)\rightarrow\mathcal{N}(0,1)$$
where $\bar{Y}_i=\frac{1}{n_i}\sum_{i=1}^{n_i}Y_{i,j}$ and $\hat{\sigma}^2_i$ is the sample variance of the $Y_{i,j}$.

\vspace{5 cm}

2. Use the approximate distribution of $\bar{Y_i}$ to write an approximate Normal-Normal conjugacy.

\vspace{3 cm}

3. Suppose we observed $Y_{i,1}, ..., Y_{i,n_i}$.  Find the posterior distribution for $\mu_i$.

\vspace{2 cm}

\newpage

4. Recall Stein's theorem that for $Y_j\sim\mathcal{N}(\mu_j,\sigma^2)$ independent for $j=1,...,K$ with $K\geq 3$, the $\mu_j$ unknown, and $\sigma^2$ known, for squared loss $\sum_{j=1}^K(\mu_j-\hat{\mu_j})^2$, the MLE $\vec{Y}$ is inadmissible.  The James Stein estimator 
$$\hat{\mu}_{JS,j}=\left(1-\frac{(K-2)\sigma^2}{\sum_{j=1}^KY_j^2}\right)Y_j$$ has strictly lower risk.  However, as before, we run into the complication that $\sigma^2$ is almost never known, and there's no reason to think the variances would be equal.  Using the approximate distribution from above, find the James Stein estimator for $\vec{\mu}$.  The estimator should make no assumptions about the variances or sample sizes being equal.

\vspace{3 cm}

5. The following simulation compares the two estimators on a small set of exponential random variables with the hierarchical mean added.  Note that the only assumption made here was that the mean parameter is distributed Normally, not that the individual observations are also Normal, that they have the same variance, or that they have the same sample size.  Remarkably, this works even with all the $n$ less than 10, quite far from $n=\infty$.

```{r, cache=T}
mu_0 <- 5
sigma_0 <- 3
nsims <- 10^5
K <- 10
ns <- c(8,9,7,9,8,9,7,8,7,9)

naive_loss <- vector(length = nsims)
js_loss <- vector(length = nsims)
for (i in 1:nsims) {
  mus <- rnorm(K, mu_0, sigma_0)
  mu_hat_naive <- vector(length = K)
  ybars <- vector(length = K)
  sigma_sqs <- vector(length = K)
  for (j in 1:K) {
    ys <- mus[j] + rexp(ns[j], 1/j) - j # Mean mu_j
    mu_hat_naive[j] <- mean(ys)
    ybars[j] <- mean(ys)
    sigma_sqs[j] <- var(ys)
  }
  # Compute JS
  mu_hat_js <- (1 - (K-2) / sum((ybars/(sqrt(sigma_sqs/ns)))^2)) * ybars
  
  # Get squared losses for both
  naive_loss[i] <- sum((mu_hat_naive - mus)^2)
  js_loss[i] <- sum((mu_hat_js - mus)^2)
}

c("Naive Risk" = mean(naive_loss), "James Stein Risk" = mean(js_loss))
```


















