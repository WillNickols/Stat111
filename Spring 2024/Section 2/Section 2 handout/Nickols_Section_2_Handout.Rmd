---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 2}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on both Google forms: [this one](https://forms.gle/BS4XXnzdR53ZhxDS9) and [this one](https://bit.ly/stat111attend).

Pset 2 due Friday 2/9.

```{r, echo=F, warning=F, message=F, cache=T}
list.of.packages <- c("ggplot2", "gridExtra")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
```

# Alphabet Soup

The following question deals with the set of four letter words in the Scrabble dictionary [available here](https://github.com/zeisler/scrabble/blob/master/db/dictionary.csv).
```{r, echo=F, cache=T}
dictionary <- read.csv("data/dictionary.csv")
```

In this question, we will be modeling the letters in four letter words as draws from a multinomial distribution: $\vv{Y}\sim\textrm{Mult}_k(4, \vv{p})$. Recall that the Multinomial PMF is $$P(Y_1=n_1,...,Y_k=n_k)=\frac{n!}{\prod_{i=1}^kn_i!}\prod_{i=1}^kp_i^{n_i}$$

1. What should $k$ be?

\vspace{1 cm}

2. Suppose we generated a draw from the distribution. What additional step would we have to perform to construct a word from the draw?

\vspace{1 cm}

3. What assumption of the Multinomial distribution is obviously violated here?

```{r, cache=T, fig.height=4.5, echo=F}
# Split words into letters
letters_out <- strsplit(dictionary$words, "")

df <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(df) <- c("letter", "position")

# Make a dataframe of letters and their positions
for (letter in letters_out) {
  df <- rbind(df, data.frame("letter" = letter, "position" = 1:4))
}

# Turn this into a table mapping letters to frequencies
df <- data.frame(table(df))
df$Freq <- df$Freq / sum(df$Freq)
df$position <- paste0("Position ", df$position)

ggplot(df, aes(x = toupper(letter), y = Freq)) + 
  geom_bar(stat="identity") + 
  xlab("Letter") + 
  ylab("Proportion") + 
  theme_bw() + 
  facet_wrap(as.factor(df$position), nrow = 4, scales = "free")
```

\vspace{3 cm}

4. Find the likelihood function $L(\vv{p};\mathbf{Y})$ where $\mathbf{Y}$ is a $n\times k$ matrix with one row for each word and one column for each letter. (E.g. "face" would become a row $1,0,1,0,1,1,0,...,0$.)  Then, find the log likelihood function. What constants can we drop?

\vspace{5 cm}

5. If we maximized the log likelihood as it stands now, we would end up with $p_j=\infty$ for all $p_j$. To prevent this, we'll restrict the sum of the $p_j$ with a [Lagrangian constraint](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint). Specifically, find the gradient of $\ell(\vv{p};\vv{y})+\lambda(1-\sum_{j=1}^kp_j)$ (the derivative with respect to each $p_j$), set it equal to 0, solve for $p_j$, and then use the fact that $\sum_{j=1}^kp_j=1$ to solve for $\lambda$. Explain in words what this MLE is.

\vspace{6 cm}

6. The following shows the $\hat{p_j}$ from the data.

```{r, cache=T, echo=F, fig.height=3, fig.align='center'}
# Split words into letters and create one long vector of all the letters
letters_out <- unlist(strsplit(dictionary$words, ""))

# Make a dataframe with the proportion of each letter
df <- data.frame(table(letters_out) / length(letters_out))
colnames(df) <- c("letters", "proportions")

ggplot(df, aes(x = toupper(letters), y = proportions)) + 
  geom_bar(stat="identity") + 
  xlab("Letter") + 
  ylab("Proportion") + 
  theme_bw()
```

7. Suppose you generate a set of 4 letters from the multinomial distribution above and put the 4 letters in a random order. What is the probability of producing the word "stat"?  Find this in two ways: Find this in two ways: first, condition on the multinomial. Second, use counting.

\vspace{4 cm}


<!-- 8. With the magic of seed setting, we can make this probability 1!  (Interestingly, the probability above means the expected number of seeds until generating "stat" is 43478, and the actual seed was 31083.) -->

<!-- ```{r, cache=T, echo=F} -->
<!-- set.seed(31083) -->

<!-- # Make a single draw from Mult(4, p) -->
<!-- draw <- as.vector(rmultinom(1, 4, df$proportions)) -->

<!-- # Turn this draw into a vector of letters -->
<!-- bag_of_letters <- vector() -->
<!-- for (i in 1:length(letters)) { -->
<!--   bag_of_letters <- c(bag_of_letters, rep(letters[i], draw[i])) -->
<!-- } -->

<!-- # Put these letters in a random order -->
<!-- word <- paste0(sample(bag_of_letters, 4), collapse = "") -->

<!-- print(word) -->
<!-- ``` -->

9. Find a maximum likelihood estimator for $\textrm{Cov}(\vv{Y}_{[1]}, \vv{Y}_{[2]})$, the covariance between the number of As and the number of Bs.

\vspace{5 cm}

10. Find a method of moments estimator for the covariance between the number of As and the number of Bs.

\vspace{5 cm}

11. Compare the MSEs of these two estimators through simulation with $n=30$ "words" in each draw.

```{r, cache=T, echo=F}
nsims <- 10000
n <- 30

single_sim <- function() {
  # Make n draws from a multinomial
  y <- rmultinom(n, 4, df$proportions)
  
  # Calculate the MLE
  mle <- -4 * rowSums(y)[1] / (4 * n) * rowSums(y)[2] / (4 * n)
  
  # Calculate the MOM estimator
  mom <- mean(y[1, ] * y[2, ]) - mean(y[1, ]) * mean(y[2, ])
  
  return (c(mle, mom))
}

# Run this nsims times, storing results in a matrix
estimates <- replicate(nsims, single_sim())

# Get the true covariance
true_cov <- - 4 * df$proportions[1] * df$proportions[2]

outputs <- c(mean((estimates[1,] - true_cov)^2),
             mean((estimates[2,] - true_cov)^2))
names(outputs) <- c("MLE MSE", "MOM MSE")

outputs
```

\vspace{2 cm}

\newpage
# Logistic Logic

The $\textrm{Logistic}(s)$ distribution is defined to be the distribution of $s\log\left(\frac{U}{1-U}\right)$ where $U\sim\textrm{Unif}(0,1)$. 

1. Find the CDF $F(y)$ of the Logistic distribution. (Hint: Let $Y$ have the Logistic distribution and let $U$ have the Uniform distribution. Then, write $Y$ in terms of $U$, isolate $U$, and use the Uniform PDF.)

\vspace{6 cm}

2. Find the PDF of the Logistic distribution.

\vspace{4 cm}

3. Let $Y$ have the Logistic distribution. Find $E(Y)$ and $\textrm{Var}(Y)$. You may use the facts that $\int_{0}^{1}\ln\left(\frac{x}{1-x}\right)dx=0$ and $\int_{0}^{1}\ln^{2}\left(\frac{x}{1-x}\right)dx=\pi^2/3$.

\vspace{8 cm}

4. Suppose we are measuring vehicle velocities on a congested highway which are distributed $\textrm{Logistic}(s)$. (It makes sense for the distribution to be symmetric around 0 since the vehicles are equally likely to be going either direction). However, our instrument can only measure velocities in the range of $[-c,c]$ (for any physicists in the room, $c$ does not represent the speed of light). We want to estimate $s$ despite this limitation. Find the likelihood function for $s$ given $n_1$ observed velocities $Y_1,...,Y_{n_1}$, $n_2$ velocities less than $-c$ and $n_3$ velocities more than $c$.

\vspace{6 cm}

5. A closed form solution for the maximum likelihood estimator of $s$ does not exist (or at least I wasn't able to find it after an hour of number pushing, and Google doesn't seem to have it either). Instead, consider the following method of moments estimator for $s$:
$$\hat{s}=\sqrt{\frac{3}{n_1\pi^2}\sum_{i=1}^{n_1}Y_i^2}$$
Describe the logic behind this estimator.

\vspace{5 cm}

6. Find the sign of the bias of $\hat{s}$ for $s$ with an argument about how $E\left(\frac{3}{n_1\pi^2}\sum_{i=1}^{n_1}Y_i^2\right)$ compares to $s^2$ and Jensen's inequality. (Since the square root function is concave, Jensen's inequality says $E(\sqrt{X})<\sqrt{E(X)}$.)

\vspace{8 cm}







