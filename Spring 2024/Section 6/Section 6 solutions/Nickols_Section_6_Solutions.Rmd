---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 6}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/BS4XXnzdR53ZhxDS9).

Pset 6...

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
```

# Prelude: Attractiveness, personality, spurious correlations, and their extensions

With this week's focus on regression, it seemed like a reasonable time to mention an idea I heard a few years ago called [Berkson's paradox](https://en.wikipedia.org/wiki/Berkson%27s_paradox). Some people say that there's a negative correlation between attractiveness and personality when looking for a romantic partner: people who are attractive can afford to be jerks, but people who aren't attractive need to be nice. However, there could be another explanation. Suppose attractiveness and personality were uncorrelated and each uniform on some 0 to 10 scale. A plot of attractiveness and personality would look like this:

```{r, echo=F, fig.height=2.5, fig.width=3, fig.align='center', cache=TRUE}
df <- data.frame(Attractiveness=runif(500, 0, 10), Personality=runif(500, 0, 10))
ggplot(df, aes(x=Personality, y=Attractiveness)) + 
  geom_point() + 
  theme_bw()
```
However, you probably wouldn't be interested in someone if they were both unattractive and a jerk. And the people who are both very attractive and very nice are probably already taken. Chop off these two corners of the graph, and voila!  Personality and attractiveness are negatively correlated!

```{r, echo=F, fig.height=2.5, fig.width=3, fig.align='center'}
df <- df[df$Attractiveness + df$Personality > 5 & df$Attractiveness + df$Personality <15,]
ggplot(df, aes(x=Personality, y=Attractiveness)) + 
  geom_point() + 
  theme_bw() + 
  geom_abline(intercept = 5, slope = -1, col="red") +
  geom_abline(intercept = 15, slope = -1, col="red")
```

The same trend holds (though to a slightly lesser extent) even if only one of the thresholds exists. Such a phenomenon can arise whenever there's a threshold that can be cleared by either of two methods and you're analyzing the correlation among the two methods in only the surviving population (research ability versus teaching ability among faculty, hard work versus intelligence among students admitted to a college, personality versus skill among people with a particular job). Importantly, the correlation implies no causation at all, and the correlation doesn't even hold when considering the full population; it's purely a result of the threshold. With that, on to some math....

\newpage

# Sine regression

It might seem like it should be impossible to fit sine functions with linear models: a sine equation isn't linear in the parameters or the data. However, a few strategic manipulations can allow linear analysis of sine functions. The following questions deal with data on the daily temperatures from Norfolk, VA [available here](https://kilthub.cmu.edu/articles/dataset/Compiled_daily_temperature_and_precipitation_data_for_the_U_S_cities/7890488?file=32874371). Let $X_i$ represent the number of days since January 1st, 1874 (the first day in the dataset) and $Y_i$ represent the maximum temperature on day $i$.

1. Suppose (extremely) naively that $Y_i=\theta_0+\theta_1 X_i+\epsilon_i$ with $\epsilon_i\sim\mathcal{N}(0,\sigma^2)$. Is this model heteroskedastic or homoskedastic?

It is homoskedastic because all the conditional variances are the same.

2. Provide numerical estimates of $\theta_0$ and $\theta_1$. How well has the model fit?

```{r, fig.height=3, fig.width=5, fig.align='center', echo=F}
# Read in data and add a days-since-Jan-1-1874 column
temps <- read.csv('data/norfolk_temps.csv')
temps$day_num <- 1:length(temps$tmax) - 1
temps <- temps[!is.na(temps$tmax),]
temps <- temps[,c("tmax", "day_num")]

```

```{r, fig.height=3, fig.width=5, fig.align='center'}
# Fit the model
naive_fit <- lm(tmax ~ day_num, temps)
summary(naive_fit)$coefficients
```

```{r, fig.height=3, fig.width=5, fig.align='center', echo=F}
# Get predicted values
predicted_df <- data.frame(tmax = naive_fit$fitted.values, day_num = temps$day_num)

# Plot real versus predicted
ggplot(temps[1:1000,], aes(x = day_num, y = tmax)) +
  geom_point() + 
  geom_line(col="red", linewidth=2, data = predicted_df[1:1000,], aes(x=day_num, y=tmax)) + 
  theme_bw() + 
  xlab("Days since January 1st, 1874") + 
  ylab("Daily high")
```

3. Provide an approximate 95\% confidence interval for how much Norfolk warms per decade on average.

Assuming approximate normality, our confidence interval will be the point estimate for $365.249\cdot10\cdot\theta_1$ (the average number of days in a decade times the change per day) plus or minus 1.96 times its standard error:

```{r}
10 * 365.249 * c("lb" = summary(naive_fit)$coefficients[2,1] - 
                   qnorm(0.975) * summary(naive_fit)$coefficients[2,2],
                 "ub" = summary(naive_fit)$coefficients[2,1] + 
                   qnorm(0.975) * summary(naive_fit)$coefficients[2,2])
```

4. Suppose someone used this interval to argue that Norfolk was experiencing climate change. Why should you be skeptical?

Though the interval is entirely above 0, the effect is very small, and something as simple as the dataset starting in (cold) January and ending in (hot) July could equally explain this trend.

5. Consider the plot of the residuals $U_i$ versus $X_i$ for the first 1000 days. What are the four linear regression assumptions?  Which are violated?

```{r, fig.height=3, echo=F}
# Calculate residuals
error_df <- data.frame(residual = temps$tmax - 
                         naive_fit$fitted.values, day_num = temps$day_num)

p1 <- ggplot(error_df[1:1000,], aes(y = residual, x = day_num)) + 
  geom_point() + 
  theme_bw() + 
  xlab("Days since January 1st, 1874") + 
  ylab("Residual")

p2 <- ggplot(error_df[1:1000,], aes(x = residual)) + 
  geom_histogram(binwidth = 5) + 
  theme_bw() + 
  ylab("Count")  +
  xlab("Residual")

grid.arrange(p1, p2, ncol=2)
```
The four assumptions in linear regression are linearity (the data actually follow a linear model), normality (the error terms are actually Normal), homoskedasticity (the error terms have equal variance), and independence (the $Y_i$ are conditionally independent given $\vv{X}$).

Linearity is definitely not upheld: there is a clear pattern in the data not explained by the linear fit. Normality is also not upheld: the tails are too small for a Normal distribution. Homoskedasticity might be upheld, but there seems to be more variability in the winter than in the rest of the year. Independence is definitely not upheld: temperature data are clearly correlated over time (if it was hot yesterday, it's more likely to be hot today).

6. Consider the model $E(Y_i|X_i)=\beta_0+\alpha\sin(2\pi\omega (X_i - \phi))+\beta X_i$. Describe what this model is saying (i.e., what each parameter means). If $\alpha, \omega, \phi,$ and $\beta$ are unknown, is this a predictive regression?  A linear regression?  Which of these variables isn't actually unknown?

This model is saying that temperatures follow a $\sin$ curve with a period $1/\omega$, an offset to the right $\phi$, an amplitude $\alpha$, and an additional change of temperature per day $\beta$. This is a predictive regression because we are aiming to find $E(Y_i|X_i=x_i)$. It is not a linear regression because we have unknown $\omega$ and $\phi$ in the $\sin$ function, so the model is not linear in the parameters. The parameter $\omega$ isn't actually unknown since we know that the period is $365.249$ days.

7. If we take the period fixed as 365.249 days, rewrite the model so it is clearly linear. Find how to determine $\phi$ and $\alpha$ from your model. The sine addition identity will be useful: $\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$.
$$Y_i=\beta_0+\alpha\sin(2\pi\omega (X_i - \phi))+\beta X_i=\beta_0+\alpha\cos(-2\pi\omega\phi)\sin(2\pi\omega X_i)+\alpha\sin(-2\pi\omega\phi)\cos(2\pi\omega X_i)+\beta X_i$$
Thus, we will have a linear regression on the known terms $\sin(2\pi\omega X_i)$, $\cos(2\pi\omega X_i)$, and $X_i$, and the coefficients will be $\beta_0=\beta_0$, $\beta_1=\beta$, $\beta_2=\alpha\cos(-2\pi\omega\phi)$, $\beta_3=\alpha\sin(-2\pi\omega\phi)$. We can find $\alpha$ with $\alpha=\sqrt{\beta_2^2+\beta_3^2}$ and $\phi=\cos^{-1}(\beta_2/\alpha)/(2\pi\omega)$.

8. We can fit this linear model and determine $\hat{\alpha}$, $\hat{\phi}$, and $\hat{\beta}$. Interpret the parameters:

```{r, fig.align='center', fig.width=5, fig.height=3, echo=F}
# Fit the model
lm_fit <- lm(tmax ~ day_num + sin(2 * pi / 365.249 * day_num) + 
               cos(2 * pi / 365.249 * day_num), temps)

# Get the predicted values from the model
predicted_df <- data.frame(tmax = lm_fit$fitted.values, day_num = temps$day_num)

# Plot the actual and predicted values
ggplot(temps[30001:31000,], aes(x=day_num, y=tmax)) + 
  geom_point() + 
  theme_bw() +
  geom_line(color = "red", linewidth = 2, data = predicted_df[30001:31000,], 
            aes(x=day_num, y=tmax)) + 
  xlab("Days since January 1st, 1874") + 
  ylab("Daily high")

# Calculate parameters
alpha <- sqrt(lm_fit$coefficients[3]^2 + lm_fit$coefficients[4]^2)
phi = acos(lm_fit$coefficients[3]/alpha) * 
  365.249 / (2 * pi)
params <- c(alpha, phi, lm_fit$coefficients[2])
names(params) <- c("alpha", "phi", "beta")

# Print parameter estimates
round(params, 7)
```

April 19th (day 109) is the offset, which corresponds to the spring day with the most average temperature of the year. The amplitude is 19.3, so the average mid-winter high is about 38.6 degrees colder than the average mid-summer high. The coefficient $\beta=4.52\cdot10^{-5}$ shows that the temperature warms by $4.52\cdot10^{-5}$ degrees per day on average.

9. Provide a new approximate 95\% confidence interval for how much Norfolk warms per decade on average. How does the rate of warming compare to the [National Oceanic and Atmospheric Administration's global estimate](https://www.climate.gov/news-features/understanding-climate/climate-change-global-temperature) of 0.14 degrees F per decade since 1880?

```{r, echo=F}
coef(summary(lm_fit))
```


```{r}
10 * 365.249 * c("lb" = summary(lm_fit)$coefficients[2,1] - qnorm(0.975) * 
                   summary(lm_fit)$coefficients[2,2],
                 "ub" = summary(lm_fit)$coefficients[2,1] + qnorm(0.975) * 
                   summary(lm_fit)$coefficients[2,2])
```

The confidence interval here slightly misses the NOAA number, but the NOAA number is for ocean and surface temperatures combined over a much larger area, so this is reasonable. Notably, this interval is about half as wide as before and not subject to the same issues with start and end points.

10. Using the plot of the residuals $U_i$ versus $X_i$ for the first 1000 days, which assumptions are violated now?

```{r, fig.height=3, echo=F}
error_df <- data.frame(residual = temps$tmax - lm_fit$fitted.values, day_num = temps$day_num)

p1 <- ggplot(error_df[1:1000,], aes(y = residual, x = day_num)) + 
  geom_point() + 
  theme_bw() + 
  xlab("Days since January 1st, 1874") + 
  ylab("Residual")

p2 <- ggplot(error_df[1:1000,], aes(x = residual)) + 
  geom_histogram(binwidth = 5) + 
  theme_bw() + 
  ylab("Count")  +
  xlab("Residual")

grid.arrange(p1, p2, ncol=2)
```

Linearity is much better now: there is much less of a pattern in the residuals. Normality is very well upheld now (a QQ plot of this data is actually one of the best I've ever seen). Homoskedasticity still seems problematic with more variability in the winter than in the rest of the year. Independence is still not upheld: even once we account for the day in the year, weather trends mean that days will be correlated (you can see this well if you reduce the number of residuals plotted).

11. Using the regression above, consider the 95% confidence interval for the conditional mean temperature on March 19th 2023 and the 95% prediction interval. How do they compare? The true high was 46. Is it surprising that one interval captured this and the other didn't?

```{r}
predict(lm_fit, 
        data.frame("day_num" = as.numeric(as.Date("2023-03-19") - as.Date("1874-01-01"))), 
        interval = "confidence", level = 0.95)
```

```{r}
predict(lm_fit, 
        data.frame("day_num" = as.numeric(as.Date("2023-03-19") - as.Date("1874-01-01"))), 
        interval = "prediction", level = 0.95)
```

Both have the same mean, but the prediction interval is much wider. As expected, this interval captured the observed temperature, but the confidence interval didn't.

\newpage

# Rule of thumb

Suppose we have $n$ pairs of $(X_i, Y_i)$ and we regress $Y$ on $X$ to get a slope $\hat{\beta}_1$ and $X$ on $Y$ to get a slope $\hat{\beta}_1'$. At first glance, it might seem like the $\hat{\beta}_1=1/\hat{\beta}_1'$. However, as you can see in the plots below, this is wrong.

```{r, echo=F, fig.width=6, fig.height=3, fig.align='center', cache=T, warning=F}
set.seed(139)
n <- 500
beta_0 <- 2
beta_1 <- 0.5
x <- rnorm(n, 10, 5)
y <- beta_0 + beta_1 * x + rnorm(n, 0, 3)
lm1 <- summary(lm(y ~ x))
plot1 <- ggplot(data.frame(x, y), aes(x=x, y=y)) + 
  geom_point() + 
  geom_smooth(method = 'lm', formula = 'y ~ x') + 
  theme_bw() + 
  annotate(
    "text", label = paste0("Slope: ", round(lm1$coefficients[2,1], 3), "\nR2: ", round(lm1$r.squared, 3)),
    x = -2, y = 25, size = 4, colour = "red"
  ) + 
  coord_equal() + 
  xlim(-8, 28) + 
  ylim(-8, 28)

lm2 <- summary(lm(x ~ y))
plot2 <- ggplot(data.frame(x, y), aes(x=y, y=x)) + 
  geom_point() + 
  geom_smooth(method = 'lm', formula = 'y ~ x') + 
  theme_bw() + 
  xlab("y") + 
  ylab("x") +
  annotate(
    "text", label = paste0("Slope: ", round(lm2$coefficients[2,1], 3), "\nR2: ", round(lm2$r.squared, 3)),
    x = -2, y = 25, size = 4, colour = "red"
  ) + 
  coord_equal() + 
  xlim(-8, 28) + 
  ylim(-8, 28)

grid.arrange(plot1, plot2, ncol=2)
```

1. Why is this wrong?

Because we are only trying to minimize the vertical residuals, we end up with non-reciprocal slopes. You can imagine a case where $X$ and $Y$ are independent, so both slopes would be 0, but clearly these are not reciprocals. This can also be viewed as a case of regression to the mean in which an extreme $X$ value predicts a $Y$ value that's not quite as extreme.

2. In the rest of the problem, we'll try to find the proper relationship between the two slopes. Recall that when regressing $Y$ on $X$, we have
$$R^2=1-\frac{\sum_{i=1}^n(Y_i-\hat{Y_i})^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}$$
Consider our simple regression with the estimators $$\hat{\beta}_1=\frac{\sum_{i=1}^n(Y_i-\bar{Y})(X_i-\bar{X})}{\sum_{i=1}^n(X_i-\bar{X})^2}, \;\;\;\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1}\bar{X}$$
and consider the flipped regression estimators $$\hat{\beta}_{1}'=\frac{\sum_{i=1}^n(Y_i-\bar{Y})(X_i-\bar{X})}{\sum_{i=1}^n(Y_i-\bar{Y})^2}, \;\;\;\hat{\beta}_{0}'=\bar{X}-\hat{\beta}_{1}'\bar{Y}$$ 
Find an expression for $\hat{\beta}_{1}'$ in terms of $\hat{\beta}_{1}$.

$$\hat{\beta}_{1}'=\hat{\beta}_{1}\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}$$

3. Solve for $R^2$ in terms of $\hat{\beta}_1$ and $\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}$. You may use the fact that $$\sum_{i=1}^n(Y_i-\bar{Y})^2=\sum_{i=1}^n(Y_i-\hat{Y_i})^2+\sum_{i=1}^n(\hat{Y_i}-\bar{Y})^2$$ (See my [Stat 111 section 6 notes from last year](https://github.com/WillNickols/Stat111/blob/main/Spring%202023/Section%206/Section%206%20solutions/Nickols_Section_6_Solutions.pdf) for why this is the case in simple linear regression.)

$$\begin{aligned}R^2&=1-\frac{\sum_{i=1}^n(Y_i-\hat{Y}_i)^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}\\
&=\frac{\sum_{i=1}^n(Y_i-\bar{Y})^2-\sum_{i=1}^n(Y_i-\hat{Y}_i)^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}\\
&=\frac{\sum_{i=1}^n(\hat{Y_i}-\bar{Y})^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}\\
&=\frac{\sum_{i=1}^n(\hat{\beta}_0+\hat{\beta}_1X_i-(\hat{\beta}_0+\hat{\beta}_1\bar{X}))^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}\\
&=\frac{\sum_{i=1}^n(\hat{\beta}_1X_i-\hat{\beta}_1\bar{X})^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}\\
&=\hat{\beta}_1^2\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2}\\
\end{aligned}$$

4. Use this to write an expression for $\hat{\beta}_1'$ in terms of $R^2$ and $\hat{\beta}_1$.

$$\hat{\beta}_1'=\frac{R^2}{\hat{\beta}_1}$$
Notably, $0\leq R^2\leq 1$ for an OLS model, so $0\leq \hat{\beta}_1'\leq 1/\hat{\beta}_1$. Not only does this give us the relation between the slopes, it does so in a way that uses the two most commonly reported statistics about the model: the estimated slope and the $R^2$!

\newpage

# Data transformations

In most regressions, right skewed variables are best transformed with a log transformation because this naturally leads to the interpretation that some constant change in the predictor results in a multiplicative change in the output. However, for moderately skewed predictors, a square root transformation can be useful to obtain a better linear model fit. Consider the following two models:

\begin{equation}
Y_i=\beta_0'+\beta_1'X_i+\epsilon_i
\end{equation}
\begin{equation}
\sqrt{Y_i}=\beta_0+\beta_1X_i+\epsilon_i
\end{equation}
for $i\in\{1,...,n\}$ with $\epsilon_i=\mathcal{N}(0,\sigma^2)$

With $X_i\sim\textrm{Unif}(0, 10)$, $n=20$, $\beta_0=5$, $\beta_1=2$, and $\sigma^2=10$, assuming the second model is correct, the following simulation finds the estimates $\hat{\beta_1}$ and $\hat{\beta_1'}$. It also estimates the following quantities: (1) the coverage probability of the 95\% confidence interval for $\mu(15)$ based on $\hat{\beta_1}$, (2) the coverage probability of the 95\% confidence interval for $\mu(15)$ based on $\hat{\beta_1'}$, (3) the coverage probability of the 95\% prediction interval for $Y_{n+1}|X_{n+1}=5$ based on $\hat{\beta_1}$, and (4) the coverage probability of the 95\% prediction interval for $Y_{n+1}|X_{n+1}=5$ based on $\hat{\beta_1'}$. Interpret the results.

```{r, cache=T}
set.seed(111)

# Parameters
n <- 20
beta_0 <- 5
beta_1 <- 2
sigma_sq <- 10
nsims <- 10^4

# Vectors for results
mu_covered <- vector(length = nsims)
mu_covered_prime <- vector(length = nsims)
new_covered <- vector(length = nsims)
new_covered_prime <- vector(length = nsims)

for (i in 1:nsims) {
  # Generate data from the model
  x <- runif(n, 0, 10)
  y <- (beta_0 + beta_1 * x + rnorm(n, 0, sqrt(sigma_sq)))^2
  
  # Fit the model on the original scale
  org_fit <- lm(y ~ x)
  
  # Fit the true model
  sqrt_fit <- lm(sqrt(y) ~ x)

  # Get the true conditional mean
  mu_true <- (beta_0 + beta_1 * 5)^2 + sigma_sq
  
  # Create an interval from beta_1 and check coverage
  mu_covered_int <- predict(sqrt_fit, data.frame(x=5), 
                               interval = "confidence", 
                               level = 0.95)^2
  mu_covered[i] <- mu_true > mu_covered_int[2] & 
    mu_true < mu_covered_int[3]
  
  # Create an interval from beta_1 and check coverage
  mu_covered_prime_int <- predict(org_fit, data.frame(x=5), 
                               interval = "confidence", 
                               level = 0.95)
  mu_covered_prime[i] <- mu_true > mu_covered_prime_int[2] & mu_true < mu_covered_prime_int[3]
  
  # Create a new data point from the model
  x_new <- 5
  y_new  <- (beta_0 + beta_1 * x_new + rnorm(1, 0, sqrt(sigma_sq)))^2
  
  # Create the intervals and check coverage
  new_covered_int <- predict(sqrt_fit, data.frame(x=x_new), 
                               interval = "prediction", 
                               level = 0.95)^2
  new_covered[i] <- y_new > new_covered_int[2] & y_new < new_covered_int[3]
  
  new_covered_prime_int <- predict(org_fit, data.frame(x=x_new), 
                               interval = "prediction", 
                               level = 0.95)
  new_covered_prime[i] <- y_new > new_covered_prime_int[2] & y_new < new_covered_prime_int[3]
  
}

df <- rbind(c(mean(mu_covered), mean(new_covered)), 
            c(mean(mu_covered_prime), mean(new_covered_prime)))
colnames(df) <- c("Confidence coverage", "Prediction coverage")
rownames(df) <- c("Beta_1", "Beta_1 prime")
knitr::kable(df)
```

Using the correct model and transforming the intervals to the original scale gives the nominal coverage probability for the prediction interval but not for the confidence interval. Fitting the wrong model on the original scale results in both being off.









